{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a731f8f1-7fde-4088-9055-29d64e770434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìò 1. Introducci√≥n\n",
    "En este notebook aprenderemos a:\n",
    "- Leer datos **estructurados** (CSV, Parquet).\n",
    "- Leer datos **semiestructurados** (JSON, anidado).\n",
    "- Manejar **esquemas expl√≠citos** para optimizaci√≥n.\n",
    "- Usar **funciones de PySpark** para transformar datos.\n",
    "- Aplicar t√©cnicas de **optimizaci√≥n en Databricks**: cache, repartition, broadcast, Delta Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d7bd735-216b-46d2-bdcb-cfbfbc7b7e58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üìÇ 2. Importaci√≥n de librer√≠as y configuraci√≥n inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20c11763-43b2-4a11-8cb8-9169091985cb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LIBRER√çAS"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7efdc105-27db-467a-824f-9d16b849b871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìä 3. Lectura de datos estructurados (CSV p√∫blico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3688edba-08f2-44df-912a-97b0f970c152",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LECTURAS CSV DESDE URL"
    }
   },
   "outputs": [],
   "source": [
    "# URL del archivo\n",
    "url = \"https://raw.githubusercontent.com/owid/covid-19-data/refs/heads/master/public/data/owid-covid-data.csv\"\n",
    "# ‚ö†Ô∏è Nota: no se puede leer directamente con spark.read.csv(url) debido a restricciones de permisos en Databricks.\n",
    "# Por eso lo cargamos primero con Pandas y luego lo convertimos a Spark DataFrame.\n",
    "\n",
    "# Leer con pandas\n",
    "pdf = pd.read_csv(url)\n",
    "\n",
    "# Convertir a Spark DataFrame\n",
    "df_tmp = spark.createDataFrame(pdf)\n",
    "\n",
    "# Definir esquema manualmente (solo las columnas que queremos)\n",
    "schema = StructType([\n",
    "    StructField(\"iso_code\", StringType(), True),\n",
    "    StructField(\"continent\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),          # puede cambiarse a DateType si se requiere\n",
    "    StructField(\"total_cases\", DoubleType(), True),\n",
    "    StructField(\"new_cases\", DoubleType(), True),\n",
    "    StructField(\"total_deaths\", DoubleType(), True),\n",
    "    StructField(\"population\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Seleccionar y castear columnas al esquema definido usando F.col\n",
    "df_covid = (\n",
    "    df_tmp\n",
    "    .select(\n",
    "        F.col(\"iso_code\").cast(StringType()),\n",
    "        F.col(\"continent\").cast(StringType()),\n",
    "        F.col(\"location\").cast(StringType()),\n",
    "        F.col(\"date\").cast(StringType()),     # o DateType si prefieres\n",
    "        F.col(\"total_cases\").cast(DoubleType()),\n",
    "        F.col(\"new_cases\").cast(DoubleType()),\n",
    "        F.col(\"total_deaths\").cast(DoubleType()),\n",
    "        F.col(\"population\").cast(DoubleType())\n",
    "    )\n",
    ")\n",
    "\n",
    "# Mostrar datos\n",
    "df_covid.show(5)\n",
    "df_covid.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf8a3a2-531c-47cb-92e0-7d0c5e2716dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üõ† 4. Transformaciones en datos estructurados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab854152-53a1-4853-98a4-b9fd719e3a33",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "TRANSFORMACI√ìN DE DATOS"
    }
   },
   "outputs": [],
   "source": [
    "df_covid = df_covid.withColumn(\"date\", F.to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "df_covid = df_covid.withColumn(\"cases_per_million\", (F.col(\"total_cases\") / F.col(\"population\")) * 1e6)\n",
    "df_colombia = df_covid.filter(F.col(\"location\") == \"Colombia\")\n",
    "display(df_colombia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87384a96-275e-4c8a-bb06-a946d8168920",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìÇ 5. Lectura de datos semiestructurados (JSON p√∫blico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22ffbf1d-4308-4d11-9070-2ba321dba03d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LECTURA DE .JSON DESDE URL"
    }
   },
   "outputs": [],
   "source": [
    "# Leer el JSON desde el volumen\n",
    "df_weather = spark.read.json(\"/Volumes/workspace/dbtest/dataclase4/weather.json\")\n",
    "\n",
    "# Mostrar esquema y datos\n",
    "df_weather.printSchema()\n",
    "df_weather.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c30649-8427-420a-bcf9-398b5d775d2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, ArrayType, StringType, DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WeatherJSON\").getOrCreate()\n",
    "\n",
    "with open(\"/Volumes/workspace/dbtest/dataclase4/weather.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"generationtime_ms\", DoubleType(), True),\n",
    "    StructField(\"hourly\", StructType([\n",
    "        StructField(\"time\", ArrayType(StringType()), True),\n",
    "        StructField(\"temperature_2m\", ArrayType(DoubleType()), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "df_weather = spark.createDataFrame([data], schema=schema)\n",
    "df_weather.printSchema()\n",
    "df_weather.show(2, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14aeebd-2b56-49c2-b9d7-8b1c826a790c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üîç 6. Manejo de datos anidados en JSON\n",
    "En la parte de arriba trabajamos con un archivo JSON que contiene informaci√≥n meteorol√≥gica. El JSON tiene datos anidados, es decir, estructuras internas como diccionarios y listas (arrays). Por ejemplo, dentro de la clave \"hourly\" hay dos arrays: \"time\" y \"temperature_2m\".\n",
    "\n",
    "Este tipo de datos requiere pasos espec√≠ficos para poder analizarlos con PySpark.\n",
    "\n",
    "Paso a Paso\n",
    "\n",
    "1. Lectura del JSON\n",
    "Primero se carga el archivo JSON en memoria, convirti√©ndolo en un diccionario de Python que se puede manipular.\n",
    "\n",
    "2. Identificaci√≥n de los datos anidados\n",
    "Se observa que dentro del JSON hay estructuras internas (diccionarios y arrays), como \"hourly\", que contiene \"time\" y \"temperature_2m\". Estos arrays representan horas y temperaturas correspondientes.\n",
    "\n",
    "3. Extracci√≥n de los arrays internos\n",
    "Para poder trabajar con ellos en Spark, se extraen los arrays de horas y temperaturas de forma separada, dejando listas planas que luego se pueden transformar en filas del DataFrame.\n",
    "\n",
    "4. Creaci√≥n de un DataFrame con esquema expl√≠cito\n",
    "Se define un esquema que indica a Spark el tipo de cada array (por ejemplo, strings para las horas y n√∫meros decimales para las temperaturas). Esto es necesario porque Spark no puede inferir autom√°ticamente los tipos de datos anidados complejos.\n",
    "\n",
    "5. Transformaci√≥n de arrays en filas individuales\n",
    "Para analizar los datos por hora, se combinan los arrays de horas y temperaturas y se ‚Äúexplota‚Äù cada par en una fila independiente. Esto convierte los datos anidados en un DataFrame plano, m√°s f√°cil de manipular y analizar.\n",
    "\n",
    "6. Agregar informaci√≥n adicional\n",
    "Se incorporan columnas adicionales como la latitud y longitud para que cada fila tenga la ubicaci√≥n asociada con cada hora y temperatura.\n",
    "\n",
    "7. Visualizaci√≥n de los resultados\n",
    "Finalmente, se observa la estructura del DataFrame y algunas filas para confirmar que los datos fueron transformados correctamente y est√°n listos para an√°lisis posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd6721f-55d0-4487-bfa9-ed6dcde24dde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType, DoubleType, StructType, StructField\n",
    "\n",
    "# Leer JSON desde archivo\n",
    "json_file_path = \"/Volumes/workspace/dbtest/dataclase4/weather.json\"\n",
    "with open(json_file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "latitude = data[\"latitude\"]\n",
    "longitude = data[\"longitude\"]\n",
    "\n",
    "# Crear DataFrame con esquema expl√≠cito\n",
    "schema = StructType([\n",
    "    StructField(\"time_array\", ArrayType(StringType()), True),\n",
    "    StructField(\"temp_array\", ArrayType(DoubleType()), True)\n",
    "])\n",
    "\n",
    "df_temp = spark.createDataFrame(\n",
    "    [(data[\"hourly\"][\"time\"], data[\"hourly\"][\"temperature_2m\"])],\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "# Combinar arrays y convertir cada par en una fila usando funciones de F\n",
    "# Selecciona y combina los arrays de horas y temperaturas\n",
    "df_weather = df_temp.select(\n",
    "    # F.arrays_zip(\"time_array\", \"temp_array\") combina los dos arrays en un solo array de structs,\n",
    "    # donde cada struct contiene un par (hora, temperatura)\n",
    "    # F.explode() \"explota\" cada struct del array en una fila separada\n",
    "    F.explode(F.arrays_zip(\"time_array\", \"temp_array\")).alias(\"row\")\n",
    ").select(\n",
    "    # F.col(\"row.time_array\") accede al campo 'time_array' dentro del struct creado por arrays_zip\n",
    "    # Se renombra la columna como 'time'\n",
    "    F.col(\"row.time_array\").alias(\"time\"),\n",
    "\n",
    "    # F.col(\"row.temp_array\") accede al campo 'temp_array' dentro del struct\n",
    "    # Se renombra la columna como 'temperature'\n",
    "    F.col(\"row.temp_array\").alias(\"temperature\")\n",
    ")\n",
    "\n",
    "# Agregar latitud y longitud usando F.lit\n",
    "df_weather = df_weather.withColumn(\"latitude\", F.lit(latitude)) \\\n",
    "                       .withColumn(\"longitude\", F.lit(longitude))\n",
    "\n",
    "# Mostrar resultados\n",
    "df_weather.printSchema()\n",
    "df_weather.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "872555ba-d336-437a-9993-5c58e85348f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ‚ö° 7. Optimizaci√≥n en Databricks / PySpark\n",
    "\n",
    "En el procesamiento distribuido con Spark, la **optimizaci√≥n** es fundamental para mejorar el rendimiento y reducir costos de c√≥mputo.  \n",
    "Aqu√≠ explicamos tres t√©cnicas clave:\n",
    "\n",
    "### üóÇ 1. `cache()` / `persist()`\n",
    "- Cuando ejecutamos transformaciones en un DataFrame, Spark no guarda los resultados inmediatamente (evaluaci√≥n diferida).  \n",
    "- Si el mismo DataFrame se usa varias veces, Spark lo recalcular√≠a cada vez.  \n",
    "- Usar `cache()` o `persist()` permite **almacenar en memoria** los resultados intermedios, evitando recomputaciones.  \n",
    "- üöÄ Beneficio: acelera consultas repetidas a costa de usar m√°s memoria.\n",
    "\n",
    "### 2. repartition() y coalesce()\n",
    "\n",
    "- Spark divide los datos en particiones, que son las unidades que se distribuyen entre los nodos del cl√∫ster.\n",
    "- repartition(n, col) ‚Üí redistribuye los datos en n particiones de forma balanceada (puede implicar un shuffle costoso).\n",
    "- coalesce(n) ‚Üí reduce el n√∫mero de particiones sin shuffle completo, √∫til cuando queremos consolidar archivos de salida.\n",
    "- üöÄ Beneficio: controlar el n√∫mero de particiones evita problemas como subutilizaci√≥n (muy pocas particiones) o sobrecarga (demasiadas particiones peque√±as).\n",
    "\n",
    "### üì° 3. broadcast()\n",
    "\n",
    "- En un join, si una de las tablas es peque√±a, se puede replicar en todos los nodos en vez de hacer un shuffle completo.\n",
    "- broadcast(df) le dice a Spark que use esa estrategia.\n",
    "- üöÄ Beneficio: reduce dr√°sticamente el tiempo y el costo de joins cuando trabajamos con tablas de referencia peque√±as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca99241-b0f0-49df-a290-67a464ae59c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Se almacena el DataFrame en cach√© para que las operaciones posteriores lo lean desde memoria\n",
    "# en lugar de recalcularlo desde la fuente original. Esto mejora el rendimiento si se reutiliza varias veces.\n",
    "df_covid.cache()\n",
    "\n",
    "# Se fuerza la materializaci√≥n del cache ejecutando una acci√≥n (count),\n",
    "# as√≠ se asegura que los datos ya queden precargados en memoria.\n",
    "df_covid.count()\n",
    "\n",
    "# Se redistribuyen los datos en 10 particiones con base en la columna \"location\".\n",
    "# Esto busca mejorar la paralelizaci√≥n y el rendimiento de futuros joins o escrituras,\n",
    "# aunque el n√∫mero fijo (10) puede no ser √≥ptimo seg√∫n el tama√±o real del dataset.\n",
    "df_covid = df_covid.repartition(10, \"location\")\n",
    "\n",
    "# Se importa la funci√≥n de broadcast para optimizar joins.\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Se seleccionan las columnas \"location\" y \"population\" de forma √∫nica.\n",
    "# Este dataset ser√° mucho m√°s peque√±o que df_covid, ideal para usarse como tabla de broadcast.\n",
    "df_pop = df_covid.select(\"location\", \"population\").distinct()\n",
    "\n",
    "# Se realiza un join entre el dataset grande (df_covid) y el peque√±o (df_pop).\n",
    "# El broadcast asegura que df_pop se copie a cada nodo, evitando un shuffle costoso.\n",
    "df_joined = df_covid.join(broadcast(df_pop), \"location\")\n",
    "\n",
    "# Se guarda df_covid en formato Delta en el path del Data Lake.\n",
    "# El modo \"overwrite\" reemplaza lo que hab√≠a antes, garantizando la versi√≥n m√°s actualizada.\n",
    "df_covid.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/covid\")\n",
    "\n",
    "# Se vuelve a leer el dataset desde el Delta Lake.\n",
    "# Esto asegura trabajar con datos consistentes y preparados para consultas posteriores.\n",
    "df_delta = spark.read.format(\"delta\").load(\"/mnt/delta/covid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a4041e6-58f3-469b-b1ca-eb1a35b330c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìà 8. Consultas anal√≠ticas (Window Functions)\n",
    "\n",
    "Las **Window Functions** en Spark permiten hacer c√°lculos avanzados sobre un conjunto de filas **relacionadas** con la fila actual, sin necesidad de agrupar toda la tabla.  \n",
    "\n",
    "üìå En otras palabras: nos permiten calcular m√©tricas como acumulados, rankings o valores anteriores/siguientes, mientras seguimos viendo todas las filas originales.\n",
    "\n",
    "### ‚öôÔ∏è ¬øC√≥mo funcionan?\n",
    "- Se definen con un **Window Specification** que indica:\n",
    "  - **PARTITION BY** ‚Üí c√≥mo dividir los datos en grupos (ej: por pa√≠s).\n",
    "  - **ORDER BY** ‚Üí c√≥mo ordenar dentro de cada grupo (ej: por fecha).\n",
    "\n",
    "### üõ† Tipos comunes de funciones de ventana:\n",
    "\n",
    "1. C√°lculo con filas anteriores o siguientes\n",
    "\n",
    "- lag(col, n) ‚Üí trae el valor de la fila anterior (n pasos atr√°s).\n",
    "- lead(col, n) ‚Üí trae el valor de la fila siguiente (n pasos adelante).\n",
    "\n",
    "2. C√°lculos acumulativos\n",
    "\n",
    "sum(), avg(), min(), max() dentro de una ventana ordenada.\n",
    "\n",
    "3. Funciones de ranking\n",
    "\n",
    "- row_number() ‚Üí numera las filas en orden.\n",
    "- rank() ‚Üí ranking con posibles empates.\n",
    "- dense_rank() ‚Üí ranking consecutivo sin saltos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81caea87-d2fc-44da-bf85-a43135d0e8fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"location\").orderBy(\"date\")\n",
    "df_covid = df_covid.withColumn(\"daily_cases\", F.col(\"total_cases\") - F.lag(\"total_cases\").over(window_spec))\n",
    "df_covid.filter(F.col(\"location\") == \"Colombia\").select(\"date\", \"daily_cases\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05646c84-62db-4e2d-84df-81f6260b925a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ‚úÖ 9. Conclusiones\n",
    "- Los **datos estructurados** (CSV, Parquet) son m√°s f√°ciles de manejar pero requieren cuidado en el **esquema** y **particiones**.\n",
    "- Los **datos semiestructurados** (JSON) necesitan transformaci√≥n de estructuras anidadas.\n",
    "- La **optimizaci√≥n** en Databricks combina t√©cnicas de Spark (cache, repartition, broadcast) con almacenamiento eficiente (**Delta Lake**)."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks_notebook_datos_estructurados_semiestructurados",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
