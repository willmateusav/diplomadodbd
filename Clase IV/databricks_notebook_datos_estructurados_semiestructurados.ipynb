{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a731f8f1-7fde-4088-9055-29d64e770434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìò 1. Introducci√≥n\n",
    "En este notebook aprenderemos a:\n",
    "- Leer datos **estructurados** (CSV, Parquet).\n",
    "- Leer datos **semiestructurados** (JSON, anidado).\n",
    "- Manejar **esquemas expl√≠citos** para optimizaci√≥n.\n",
    "- Usar **funciones de PySpark** para transformar datos.\n",
    "- Aplicar t√©cnicas de **optimizaci√≥n en Databricks**: cache, repartition, broadcast, Delta Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d7bd735-216b-46d2-bdcb-cfbfbc7b7e58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üìÇ 2. Importaci√≥n de librer√≠as y configuraci√≥n inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20c11763-43b2-4a11-8cb8-9169091985cb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LIBRER√çAS"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7efdc105-27db-467a-824f-9d16b849b871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìä 3. Lectura de datos estructurados (CSV p√∫blico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3688edba-08f2-44df-912a-97b0f970c152",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LECTURAS CSV DESDE URL"
    }
   },
   "outputs": [],
   "source": [
    "# URL del archivo\n",
    "url = \"https://raw.githubusercontent.com/owid/covid-19-data/refs/heads/master/public/data/owid-covid-data.csv\"\n",
    "# ‚ö†Ô∏è Nota: no se puede leer directamente con spark.read.csv(url) debido a restricciones de permisos en Databricks.\n",
    "# Por eso lo cargamos primero con Pandas y luego lo convertimos a Spark DataFrame.\n",
    "\n",
    "# Leer con pandas\n",
    "pdf = pd.read_csv(url)\n",
    "\n",
    "# Convertir a Spark DataFrame\n",
    "df_tmp = spark.createDataFrame(pdf)\n",
    "\n",
    "# Definir esquema manualmente (solo las columnas que queremos)\n",
    "schema = StructType([\n",
    "    StructField(\"iso_code\", StringType(), True),\n",
    "    StructField(\"continent\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),          # puede cambiarse a DateType si se requiere\n",
    "    StructField(\"total_cases\", DoubleType(), True),\n",
    "    StructField(\"new_cases\", DoubleType(), True),\n",
    "    StructField(\"total_deaths\", DoubleType(), True),\n",
    "    StructField(\"population\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Seleccionar y castear columnas al esquema definido usando F.col\n",
    "df_covid = (\n",
    "    df_tmp\n",
    "    .select(\n",
    "        F.col(\"iso_code\").cast(StringType()),\n",
    "        F.col(\"continent\").cast(StringType()),\n",
    "        F.col(\"location\").cast(StringType()),\n",
    "        F.col(\"date\").cast(StringType()),     # o DateType si prefieres\n",
    "        F.col(\"total_cases\").cast(DoubleType()),\n",
    "        F.col(\"new_cases\").cast(DoubleType()),\n",
    "        F.col(\"total_deaths\").cast(DoubleType()),\n",
    "        F.col(\"population\").cast(DoubleType())\n",
    "    )\n",
    ")\n",
    "\n",
    "# Mostrar datos\n",
    "df_covid.show(5)\n",
    "df_covid.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf8a3a2-531c-47cb-92e0-7d0c5e2716dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üõ† 4. Transformaciones en datos estructurados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab854152-53a1-4853-98a4-b9fd719e3a33",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "TRANSFORMACI√ìN DE DATOS"
    }
   },
   "outputs": [],
   "source": [
    "df_covid = df_covid.withColumn(\"date\", F.to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "df_covid = df_covid.withColumn(\"cases_per_million\", (F.col(\"total_cases\") / F.col(\"population\")) * 1e6)\n",
    "df_colombia = df_covid.filter(F.col(\"location\") == \"Colombia\")\n",
    "display(df_colombia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87384a96-275e-4c8a-bb06-a946d8168920",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìÇ 5. Lectura de datos semiestructurados (JSON p√∫blico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22ffbf1d-4308-4d11-9070-2ba321dba03d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LECTURA DE .JSON DESDE URL"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# URL del API\n",
    "json_url = \"https://api.open-meteo.com/v1/forecast?latitude=6.2442&longitude=-75.5812&hourly=temperature_2m\"\n",
    "# ‚ö†Ô∏è No se puede leer directamente con spark.read.json(json_url) por restricciones en Databricks.\n",
    "# Por eso descargamos primero con requests y guardamos local.\n",
    "\n",
    "# Descargar el JSON\n",
    "response = requests.get(json_url)\n",
    "data = response.json()\n",
    "\n",
    "# Guardar en un archivo temporal\n",
    "local_path = \"/tmp/weather.json\"\n",
    "with open(local_path, \"w\") as f:\n",
    "    json.dump(data, f)\n",
    "\n",
    "# Cargar el JSON con Spark\n",
    "df_weather = spark.read.json(\"file:\" + local_path)\n",
    "\n",
    "df_weather.printSchema()\n",
    "df_weather.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14aeebd-2b56-49c2-b9d7-8b1c826a790c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üîç 6. Manejo de datos anidados en JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b8ed628-4174-497c-a2d4-d3532a41e8ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_weather_expanded = df_weather.select(\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"hourly.time\",\n",
    "    \"hourly.temperature_2m\"\n",
    ")\n",
    "df_weather_expanded.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "872555ba-d336-437a-9993-5c58e85348f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ‚ö° 7. Optimizaci√≥n en Databricks / PySpark\n",
    "\n",
    "En el procesamiento distribuido con Spark, la **optimizaci√≥n** es fundamental para mejorar el rendimiento y reducir costos de c√≥mputo.  \n",
    "Aqu√≠ explicamos tres t√©cnicas clave:\n",
    "\n",
    "### üóÇ 1. `cache()` / `persist()`\n",
    "- Cuando ejecutamos transformaciones en un DataFrame, Spark no guarda los resultados inmediatamente (evaluaci√≥n diferida).  \n",
    "- Si el mismo DataFrame se usa varias veces, Spark lo recalcular√≠a cada vez.  \n",
    "- Usar `cache()` o `persist()` permite **almacenar en memoria** los resultados intermedios, evitando recomputaciones.  \n",
    "- üöÄ Beneficio: acelera consultas repetidas a costa de usar m√°s memoria.\n",
    "\n",
    "### 2. repartition() y coalesce()\n",
    "\n",
    "- Spark divide los datos en particiones, que son las unidades que se distribuyen entre los nodos del cl√∫ster.\n",
    "- repartition(n, col) ‚Üí redistribuye los datos en n particiones de forma balanceada (puede implicar un shuffle costoso).\n",
    "- coalesce(n) ‚Üí reduce el n√∫mero de particiones sin shuffle completo, √∫til cuando queremos consolidar archivos de salida.\n",
    "- üöÄ Beneficio: controlar el n√∫mero de particiones evita problemas como subutilizaci√≥n (muy pocas particiones) o sobrecarga (demasiadas particiones peque√±as).\n",
    "\n",
    "### üì° 3. broadcast()\n",
    "\n",
    "- En un join, si una de las tablas es peque√±a, se puede replicar en todos los nodos en vez de hacer un shuffle completo.\n",
    "- broadcast(df) le dice a Spark que use esa estrategia.\n",
    "- üöÄ Beneficio: reduce dr√°sticamente el tiempo y el costo de joins cuando trabajamos con tablas de referencia peque√±as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca99241-b0f0-49df-a290-67a464ae59c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_covid.cache()\n",
    "df_covid.count()\n",
    "\n",
    "df_covid = df_covid.repartition(10, \"location\")\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "df_pop = df_covid.select(\"location\", \"population\").distinct()\n",
    "df_joined = df_covid.join(broadcast(df_pop), \"location\")\n",
    "\n",
    "df_covid.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/covid\")\n",
    "df_delta = spark.read.format(\"delta\").load(\"/mnt/delta/covid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a4041e6-58f3-469b-b1ca-eb1a35b330c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìà 8. Consultas anal√≠ticas (Window Functions)\n",
    "\n",
    "Las **Window Functions** en Spark permiten hacer c√°lculos avanzados sobre un conjunto de filas **relacionadas** con la fila actual, sin necesidad de agrupar toda la tabla.  \n",
    "\n",
    "üìå En otras palabras: nos permiten calcular m√©tricas como acumulados, rankings o valores anteriores/siguientes, mientras seguimos viendo todas las filas originales.\n",
    "\n",
    "### ‚öôÔ∏è ¬øC√≥mo funcionan?\n",
    "- Se definen con un **Window Specification** que indica:\n",
    "  - **PARTITION BY** ‚Üí c√≥mo dividir los datos en grupos (ej: por pa√≠s).\n",
    "  - **ORDER BY** ‚Üí c√≥mo ordenar dentro de cada grupo (ej: por fecha).\n",
    "\n",
    "### üõ† Tipos comunes de funciones de ventana:\n",
    "\n",
    "1. C√°lculo con filas anteriores o siguientes\n",
    "\n",
    "- lag(col, n) ‚Üí trae el valor de la fila anterior (n pasos atr√°s).\n",
    "- lead(col, n) ‚Üí trae el valor de la fila siguiente (n pasos adelante).\n",
    "\n",
    "2. C√°lculos acumulativos\n",
    "\n",
    "sum(), avg(), min(), max() dentro de una ventana ordenada.\n",
    "\n",
    "3. Funciones de ranking\n",
    "\n",
    "- row_number() ‚Üí numera las filas en orden.\n",
    "- rank() ‚Üí ranking con posibles empates.\n",
    "- dense_rank() ‚Üí ranking consecutivo sin saltos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81caea87-d2fc-44da-bf85-a43135d0e8fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"location\").orderBy(\"date\")\n",
    "df_covid = df_covid.withColumn(\"daily_cases\", F.col(\"total_cases\") - F.lag(\"total_cases\").over(window_spec))\n",
    "df_covid.filter(F.col(\"location\") == \"Colombia\").select(\"date\", \"daily_cases\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05646c84-62db-4e2d-84df-81f6260b925a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ‚úÖ 9. Conclusiones\n",
    "- Los **datos estructurados** (CSV, Parquet) son m√°s f√°ciles de manejar pero requieren cuidado en el **esquema** y **particiones**.\n",
    "- Los **datos semiestructurados** (JSON) necesitan transformaci√≥n de estructuras anidadas.\n",
    "- La **optimizaci√≥n** en Databricks combina t√©cnicas de Spark (cache, repartition, broadcast) con almacenamiento eficiente (**Delta Lake**)."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks_notebook_datos_estructurados_semiestructurados",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
