{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a731f8f1-7fde-4088-9055-29d64e770434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 📘 1. Introducción\n",
    "En este notebook aprenderemos a:\n",
    "- Leer datos **estructurados** (CSV, Parquet).\n",
    "- Leer datos **semiestructurados** (JSON, anidado).\n",
    "- Manejar **esquemas explícitos** para optimización.\n",
    "- Usar **funciones de PySpark** para transformar datos.\n",
    "- Aplicar técnicas de **optimización en Databricks**: cache, repartition, broadcast, Delta Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d7bd735-216b-46d2-bdcb-cfbfbc7b7e58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "📂 2. Importación de librerías y configuración inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20c11763-43b2-4a11-8cb8-9169091985cb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LIBRERÍAS"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7efdc105-27db-467a-824f-9d16b849b871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 📊 3. Lectura de datos estructurados (CSV público)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3688edba-08f2-44df-912a-97b0f970c152",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LECTURAS CSV DESDE URL"
    }
   },
   "outputs": [],
   "source": [
    "# URL del archivo\n",
    "url = \"https://raw.githubusercontent.com/owid/covid-19-data/refs/heads/master/public/data/owid-covid-data.csv\"\n",
    "# ⚠️ Nota: no se puede leer directamente con spark.read.csv(url) debido a restricciones de permisos en Databricks.\n",
    "# Por eso lo cargamos primero con Pandas y luego lo convertimos a Spark DataFrame.\n",
    "\n",
    "# Leer con pandas\n",
    "pdf = pd.read_csv(url)\n",
    "\n",
    "# Convertir a Spark DataFrame\n",
    "df_tmp = spark.createDataFrame(pdf)\n",
    "\n",
    "# Definir esquema manualmente (solo las columnas que queremos)\n",
    "schema = StructType([\n",
    "    StructField(\"iso_code\", StringType(), True),\n",
    "    StructField(\"continent\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),          # puede cambiarse a DateType si se requiere\n",
    "    StructField(\"total_cases\", DoubleType(), True),\n",
    "    StructField(\"new_cases\", DoubleType(), True),\n",
    "    StructField(\"total_deaths\", DoubleType(), True),\n",
    "    StructField(\"population\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Seleccionar y castear columnas al esquema definido usando F.col\n",
    "df_covid = (\n",
    "    df_tmp\n",
    "    .select(\n",
    "        F.col(\"iso_code\").cast(StringType()),\n",
    "        F.col(\"continent\").cast(StringType()),\n",
    "        F.col(\"location\").cast(StringType()),\n",
    "        F.col(\"date\").cast(StringType()),     # o DateType si prefieres\n",
    "        F.col(\"total_cases\").cast(DoubleType()),\n",
    "        F.col(\"new_cases\").cast(DoubleType()),\n",
    "        F.col(\"total_deaths\").cast(DoubleType()),\n",
    "        F.col(\"population\").cast(DoubleType())\n",
    "    )\n",
    ")\n",
    "\n",
    "# Mostrar datos\n",
    "df_covid.show(5)\n",
    "df_covid.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf8a3a2-531c-47cb-92e0-7d0c5e2716dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🛠 4. Transformaciones en datos estructurados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab854152-53a1-4853-98a4-b9fd719e3a33",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "TRANSFORMACIÓN DE DATOS"
    }
   },
   "outputs": [],
   "source": [
    "df_covid = df_covid.withColumn(\"date\", F.to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "df_covid = df_covid.withColumn(\"cases_per_million\", (F.col(\"total_cases\") / F.col(\"population\")) * 1e6)\n",
    "df_colombia = df_covid.filter(F.col(\"location\") == \"Colombia\")\n",
    "display(df_colombia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87384a96-275e-4c8a-bb06-a946d8168920",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 📂 5. Lectura de datos semiestructurados (JSON público)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22ffbf1d-4308-4d11-9070-2ba321dba03d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LECTURA DE .JSON DESDE URL"
    }
   },
   "outputs": [],
   "source": [
    "# Leer el JSON desde el volumen\n",
    "df_weather = spark.read.json(\"/Volumes/workspace/dbtest/dataclase4/weather.json\")\n",
    "\n",
    "# Mostrar esquema y datos\n",
    "df_weather.printSchema()\n",
    "df_weather.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c30649-8427-420a-bcf9-398b5d775d2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, ArrayType, StringType, DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WeatherJSON\").getOrCreate()\n",
    "\n",
    "with open(\"/Volumes/workspace/dbtest/dataclase4/weather.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"generationtime_ms\", DoubleType(), True),\n",
    "    StructField(\"hourly\", StructType([\n",
    "        StructField(\"time\", ArrayType(StringType()), True),\n",
    "        StructField(\"temperature_2m\", ArrayType(DoubleType()), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "df_weather = spark.createDataFrame([data], schema=schema)\n",
    "df_weather.printSchema()\n",
    "df_weather.show(2, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14aeebd-2b56-49c2-b9d7-8b1c826a790c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🔍 6. Manejo de datos anidados en JSON\n",
    "En la parte de arriba trabajamos con un archivo JSON que contiene información meteorológica. El JSON tiene datos anidados, es decir, estructuras internas como diccionarios y listas (arrays). Por ejemplo, dentro de la clave \"hourly\" hay dos arrays: \"time\" y \"temperature_2m\".\n",
    "\n",
    "Este tipo de datos requiere pasos específicos para poder analizarlos con PySpark.\n",
    "\n",
    "Paso a Paso\n",
    "\n",
    "1. Lectura del JSON\n",
    "Primero se carga el archivo JSON en memoria, convirtiéndolo en un diccionario de Python que se puede manipular.\n",
    "\n",
    "2. Identificación de los datos anidados\n",
    "Se observa que dentro del JSON hay estructuras internas (diccionarios y arrays), como \"hourly\", que contiene \"time\" y \"temperature_2m\". Estos arrays representan horas y temperaturas correspondientes.\n",
    "\n",
    "3. Extracción de los arrays internos\n",
    "Para poder trabajar con ellos en Spark, se extraen los arrays de horas y temperaturas de forma separada, dejando listas planas que luego se pueden transformar en filas del DataFrame.\n",
    "\n",
    "4. Creación de un DataFrame con esquema explícito\n",
    "Se define un esquema que indica a Spark el tipo de cada array (por ejemplo, strings para las horas y números decimales para las temperaturas). Esto es necesario porque Spark no puede inferir automáticamente los tipos de datos anidados complejos.\n",
    "\n",
    "5. Transformación de arrays en filas individuales\n",
    "Para analizar los datos por hora, se combinan los arrays de horas y temperaturas y se “explota” cada par en una fila independiente. Esto convierte los datos anidados en un DataFrame plano, más fácil de manipular y analizar.\n",
    "\n",
    "6. Agregar información adicional\n",
    "Se incorporan columnas adicionales como la latitud y longitud para que cada fila tenga la ubicación asociada con cada hora y temperatura.\n",
    "\n",
    "7. Visualización de los resultados\n",
    "Finalmente, se observa la estructura del DataFrame y algunas filas para confirmar que los datos fueron transformados correctamente y están listos para análisis posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd6721f-55d0-4487-bfa9-ed6dcde24dde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType, DoubleType, StructType, StructField\n",
    "\n",
    "# Leer JSON desde archivo\n",
    "json_file_path = \"/Volumes/workspace/dbtest/dataclase4/weather.json\"\n",
    "with open(json_file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "latitude = data[\"latitude\"]\n",
    "longitude = data[\"longitude\"]\n",
    "\n",
    "# Crear DataFrame con esquema explícito\n",
    "schema = StructType([\n",
    "    StructField(\"time_array\", ArrayType(StringType()), True),\n",
    "    StructField(\"temp_array\", ArrayType(DoubleType()), True)\n",
    "])\n",
    "\n",
    "df_temp = spark.createDataFrame(\n",
    "    [(data[\"hourly\"][\"time\"], data[\"hourly\"][\"temperature_2m\"])],\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "# Combinar arrays y convertir cada par en una fila usando funciones de F\n",
    "# Selecciona y combina los arrays de horas y temperaturas\n",
    "df_weather = df_temp.select(\n",
    "    # F.arrays_zip(\"time_array\", \"temp_array\") combina los dos arrays en un solo array de structs,\n",
    "    # donde cada struct contiene un par (hora, temperatura)\n",
    "    # F.explode() \"explota\" cada struct del array en una fila separada\n",
    "    F.explode(F.arrays_zip(\"time_array\", \"temp_array\")).alias(\"row\")\n",
    ").select(\n",
    "    # F.col(\"row.time_array\") accede al campo 'time_array' dentro del struct creado por arrays_zip\n",
    "    # Se renombra la columna como 'time'\n",
    "    F.col(\"row.time_array\").alias(\"time\"),\n",
    "\n",
    "    # F.col(\"row.temp_array\") accede al campo 'temp_array' dentro del struct\n",
    "    # Se renombra la columna como 'temperature'\n",
    "    F.col(\"row.temp_array\").alias(\"temperature\")\n",
    ")\n",
    "\n",
    "# Agregar latitud y longitud usando F.lit\n",
    "df_weather = df_weather.withColumn(\"latitude\", F.lit(latitude)) \\\n",
    "                       .withColumn(\"longitude\", F.lit(longitude))\n",
    "\n",
    "# Mostrar resultados\n",
    "df_weather.printSchema()\n",
    "df_weather.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "872555ba-d336-437a-9993-5c58e85348f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ⚡ 7. Optimización en Databricks / PySpark\n",
    "\n",
    "En el procesamiento distribuido con Spark, la **optimización** es fundamental para mejorar el rendimiento y reducir costos de cómputo.  \n",
    "Aquí explicamos tres técnicas clave:\n",
    "\n",
    "### 🗂 1. `cache()` / `persist()`\n",
    "- Cuando ejecutamos transformaciones en un DataFrame, Spark no guarda los resultados inmediatamente (evaluación diferida).  \n",
    "- Si el mismo DataFrame se usa varias veces, Spark lo recalcularía cada vez.  \n",
    "- Usar `cache()` o `persist()` permite **almacenar en memoria** los resultados intermedios, evitando recomputaciones.  \n",
    "- 🚀 Beneficio: acelera consultas repetidas a costa de usar más memoria.\n",
    "\n",
    "### 2. repartition() y coalesce()\n",
    "\n",
    "- Spark divide los datos en particiones, que son las unidades que se distribuyen entre los nodos del clúster.\n",
    "- repartition(n, col) → redistribuye los datos en n particiones de forma balanceada (puede implicar un shuffle costoso).\n",
    "- coalesce(n) → reduce el número de particiones sin shuffle completo, útil cuando queremos consolidar archivos de salida.\n",
    "- 🚀 Beneficio: controlar el número de particiones evita problemas como subutilización (muy pocas particiones) o sobrecarga (demasiadas particiones pequeñas).\n",
    "\n",
    "### 📡 3. broadcast()\n",
    "\n",
    "- En un join, si una de las tablas es pequeña, se puede replicar en todos los nodos en vez de hacer un shuffle completo.\n",
    "- broadcast(df) le dice a Spark que use esa estrategia.\n",
    "- 🚀 Beneficio: reduce drásticamente el tiempo y el costo de joins cuando trabajamos con tablas de referencia pequeñas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca99241-b0f0-49df-a290-67a464ae59c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Se almacena el DataFrame en caché para que las operaciones posteriores lo lean desde memoria\n",
    "# en lugar de recalcularlo desde la fuente original. Esto mejora el rendimiento si se reutiliza varias veces.\n",
    "df_covid.cache()\n",
    "\n",
    "# Se fuerza la materialización del cache ejecutando una acción (count),\n",
    "# así se asegura que los datos ya queden precargados en memoria.\n",
    "df_covid.count()\n",
    "\n",
    "# Se redistribuyen los datos en 10 particiones con base en la columna \"location\".\n",
    "# Esto busca mejorar la paralelización y el rendimiento de futuros joins o escrituras,\n",
    "# aunque el número fijo (10) puede no ser óptimo según el tamaño real del dataset.\n",
    "df_covid = df_covid.repartition(10, \"location\")\n",
    "\n",
    "# Se importa la función de broadcast para optimizar joins.\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Se seleccionan las columnas \"location\" y \"population\" de forma única.\n",
    "# Este dataset será mucho más pequeño que df_covid, ideal para usarse como tabla de broadcast.\n",
    "df_pop = df_covid.select(\"location\", \"population\").distinct()\n",
    "\n",
    "# Se realiza un join entre el dataset grande (df_covid) y el pequeño (df_pop).\n",
    "# El broadcast asegura que df_pop se copie a cada nodo, evitando un shuffle costoso.\n",
    "df_joined = df_covid.join(broadcast(df_pop), \"location\")\n",
    "\n",
    "# Se guarda df_covid en formato Delta en el path del Data Lake.\n",
    "# El modo \"overwrite\" reemplaza lo que había antes, garantizando la versión más actualizada.\n",
    "df_covid.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/covid\")\n",
    "\n",
    "# Se vuelve a leer el dataset desde el Delta Lake.\n",
    "# Esto asegura trabajar con datos consistentes y preparados para consultas posteriores.\n",
    "df_delta = spark.read.format(\"delta\").load(\"/mnt/delta/covid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a4041e6-58f3-469b-b1ca-eb1a35b330c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 📈 8. Consultas analíticas (Window Functions)\n",
    "\n",
    "Las **Window Functions** en Spark permiten hacer cálculos avanzados sobre un conjunto de filas **relacionadas** con la fila actual, sin necesidad de agrupar toda la tabla.  \n",
    "\n",
    "📌 En otras palabras: nos permiten calcular métricas como acumulados, rankings o valores anteriores/siguientes, mientras seguimos viendo todas las filas originales.\n",
    "\n",
    "### ⚙️ ¿Cómo funcionan?\n",
    "- Se definen con un **Window Specification** que indica:\n",
    "  - **PARTITION BY** → cómo dividir los datos en grupos (ej: por país).\n",
    "  - **ORDER BY** → cómo ordenar dentro de cada grupo (ej: por fecha).\n",
    "\n",
    "### 🛠 Tipos comunes de funciones de ventana:\n",
    "\n",
    "1. Cálculo con filas anteriores o siguientes\n",
    "\n",
    "- lag(col, n) → trae el valor de la fila anterior (n pasos atrás).\n",
    "- lead(col, n) → trae el valor de la fila siguiente (n pasos adelante).\n",
    "\n",
    "2. Cálculos acumulativos\n",
    "\n",
    "sum(), avg(), min(), max() dentro de una ventana ordenada.\n",
    "\n",
    "3. Funciones de ranking\n",
    "\n",
    "- row_number() → numera las filas en orden.\n",
    "- rank() → ranking con posibles empates.\n",
    "- dense_rank() → ranking consecutivo sin saltos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81caea87-d2fc-44da-bf85-a43135d0e8fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"location\").orderBy(\"date\")\n",
    "df_covid = df_covid.withColumn(\"daily_cases\", F.col(\"total_cases\") - F.lag(\"total_cases\").over(window_spec))\n",
    "df_covid.filter(F.col(\"location\") == \"Colombia\").select(\"date\", \"daily_cases\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05646c84-62db-4e2d-84df-81f6260b925a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ✅ 9. Conclusiones\n",
    "- Los **datos estructurados** (CSV, Parquet) son más fáciles de manejar pero requieren cuidado en el **esquema** y **particiones**.\n",
    "- Los **datos semiestructurados** (JSON) necesitan transformación de estructuras anidadas.\n",
    "- La **optimización** en Databricks combina técnicas de Spark (cache, repartition, broadcast) con almacenamiento eficiente (**Delta Lake**)."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks_notebook_datos_estructurados_semiestructurados",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
