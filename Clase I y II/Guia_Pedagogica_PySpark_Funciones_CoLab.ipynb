{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15cf99c5-dc1d-4ab2-82e7-2a2c92ee5be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "BDmMmD4egC24"
   },
   "source": [
    "# 🚀 Instalación y configuración de PySpark en Google Colab\n",
    "En esta primera parte vamos a instalar **PySpark** en Google Colab.  \n",
    "Esto nos permitirá simular un entorno distribuido de procesamiento de datos, sin necesidad de montar un cluster real.  \n",
    "\n",
    "Spark funciona bajo una arquitectura **Driver–Workers**:  \n",
    "- **Driver:** coordina el trabajo.  \n",
    "- **Workers:** procesan partes de los datos en paralelo.  \n",
    "\n",
    "👉 Con estas líneas de código, tendremos un mini-cluster Spark en Colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4137821-5fca-48a8-bbd6-146742b1bab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "CbSy7Rdjlott"
   },
   "source": [
    "# 🚀 SparkSession y funciones `F` en PySpark\n",
    "\n",
    "## 🔹 SparkSession\n",
    "En PySpark, el **punto de entrada principal** para trabajar con datos es la **SparkSession**.  \n",
    "\n",
    "- **En entornos locales (tu computador o un servidor sin Spark preconfigurado):**  \n",
    "  Necesitamos **crear la sesión** manualmente, indicando un nombre para nuestra aplicación.  \n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ClaseBigData\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e605420-ebc0-43e9-802b-bcae2ec3f8bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_5q5hs1fWNy",
    "outputId": "094ca628-8797-4135-c9dd-746148638d2f"
   },
   "outputs": [],
   "source": [
    "# Instalamos PySpark\n",
    "!pip install pyspark\n",
    "\n",
    "# Importamos librerías\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Creamos la sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"ClaseBigData\").getOrCreate()\n",
    "\n",
    "# Verificamos la versión\n",
    "print(\"Versión de Spark:\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c4cfa7e-34b2-4857-b646-39f11ccca913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Bc_DP-CeqEL_"
   },
   "source": [
    "# 📦 `pyspark.sql.functions as F`\n",
    "\n",
    "## 🔹 ¿Qué es `F`?\n",
    "En PySpark, la librería `pyspark.sql.functions` (comúnmente importada como `F`) es un **conjunto de funciones listas para usar** que permiten manipular columnas de un **DataFrame distribuido**.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d34078a-45a0-40ef-9984-cec0001fc850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "X_vb8ppYqoHQ"
   },
   "source": [
    "# 📌 Funciones más usadas en PySpark (`pyspark.sql.functions as F`)\n",
    "\n",
    "A continuación se presentan las principales categorías de funciones de `F` con ejemplos de uso en PySpark:\n",
    "\n",
    "# Guía de Funciones PySpark\n",
    "\n",
    "## 1. 📌 Operaciones sobre columnas\n",
    "\n",
    "Permiten crear, seleccionar o condicionar columnas.\n",
    "\n",
    "```python\n",
    "# Seleccionar una columna\n",
    "F.col(\"columna\")\n",
    "\n",
    "# Crear una columna con un valor fijo\n",
    "F.lit(100)\n",
    "\n",
    "# Condicional tipo if-else\n",
    "F.when(F.col(\"precio\") > 100, F.col(\"precio\") * 0.9).otherwise(F.col(\"precio\"))\n",
    "```\n",
    "\n",
    "**Ejemplo de uso:**\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"con_descuento\",\n",
    "                   F.when(F.col(\"precio\") > 100, F.col(\"precio\") * 0.9)\n",
    "                    .otherwise(F.col(\"precio\")))\n",
    "```\n",
    "\n",
    "## 2. 📊 Agregaciones\n",
    "\n",
    "Se usan junto a `groupBy()` y `agg()` para resumir información.\n",
    "\n",
    "```python\n",
    "F.count(\"*\")                # Contar filas\n",
    "F.sum(\"columna\")            # Suma\n",
    "F.avg(\"columna\")            # Promedio\n",
    "F.max(\"columna\")            # Máximo\n",
    "F.min(\"columna\")            # Mínimo\n",
    "F.countDistinct(\"columna\")  # Valores únicos\n",
    "```\n",
    "\n",
    "**Ejemplo de uso:**\n",
    "\n",
    "```python\n",
    "df.groupBy(\"categoria\").agg(F.avg(\"precio\").alias(\"precio_promedio\"))\n",
    "```\n",
    "\n",
    "**Comparación en Python:**\n",
    "\n",
    "```python\n",
    "df.groupby(\"categoria\")[\"precio\"].mean()\n",
    "```\n",
    "\n",
    "## 3. 🔤 Funciones de texto\n",
    "\n",
    "Se utilizan para limpiar o manipular cadenas.\n",
    "\n",
    "```python\n",
    "F.upper(\"columna\")                                # Convertir a mayúsculas\n",
    "F.lower(\"columna\")                                # Convertir a minúsculas\n",
    "F.length(\"columna\")                               # Longitud de la cadena\n",
    "F.concat(F.col(\"nombre\"), F.lit(\" \"), F.col(\"apellido\"))  # Concatenar\n",
    "```\n",
    "\n",
    "**Ejemplo de uso:**\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"nombre_completo\",\n",
    "                   F.concat(F.col(\"nombre\"), F.lit(\" \"), F.col(\"apellido\")))\n",
    "```\n",
    "\n",
    "**Comparación en Python:**\n",
    "\n",
    "```python\n",
    "df[\"nombre_completo\"] = df[\"nombre\"] + \" \" + df[\"apellido\"]\n",
    "```\n",
    "\n",
    "## 4. 📅 Funciones de fechas y tiempos\n",
    "\n",
    "Son clave para análisis temporal.\n",
    "\n",
    "```python\n",
    "F.current_date()                           # Fecha actual\n",
    "F.current_timestamp()                      # Fecha y hora actual\n",
    "F.year(\"columna_fecha\")                    # Extraer año\n",
    "F.month(\"columna_fecha\")                   # Extraer mes\n",
    "F.dayofmonth(\"columna_fecha\")              # Extraer día del mes\n",
    "F.datediff(F.current_date(), \"columna_fecha\")  # Diferencia en días\n",
    "```\n",
    "\n",
    "**Ejemplo de uso:**\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"edad_dias\",\n",
    "                   F.datediff(F.current_date(), F.col(\"fecha_nacimiento\")))\n",
    "```\n",
    "\n",
    "**Comparación en Python:**\n",
    "\n",
    "```python\n",
    "(df[\"fecha_actual\"] - df[\"fecha_nacimiento\"]).dt.days\n",
    "```\n",
    "\n",
    "## 5. ⚠️ Manejo de nulos\n",
    "\n",
    "Muy importantes para limpiar datos faltantes.\n",
    "\n",
    "```python\n",
    "F.isnull(\"columna\")                 # Detectar nulos True/False\n",
    "F.coalesce(\"columna1\", \"columna2\", \"columna3\", ...)   # Por cada fila, evalúa las columnas en orden y devuelve el primer valor que no sea nulo\n",
    "```\n",
    "\n",
    "**Ejemplo de uso:**\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"valor_final\", F.coalesce(\"columnaA\", \"columnaB\"))\n",
    "```\n",
    "\n",
    "**Comparación en Python:**\n",
    "\n",
    "```python\n",
    "df[\"valor_final\"] = df[\"columnaA\"].fillna(df[\"columnaB\"])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "398f2b71-4751-4d0f-a6bf-43f9a7394daa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "JiwDeVFzhXy_"
   },
   "source": [
    "📂 Cargando un CSV masivo en Spark  \n",
    "En esta sección vamos a cargar un dataset de **15,747,461 registros**, que en Excel o pandas sería prácticamente imposible de manejar.  \n",
    "\n",
    "Spark puede leer este archivo en paralelo, lo que significa que lo divide en **particiones** y asigna cada fragmento a un *worker*.  \n",
    "De esta manera, el procesamiento es mucho más eficiente que con herramientas tradicionales.  \n",
    "\n",
    "👉 Usaremos un CSV de 15,7 millones de filas con el siguiente esquema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f1d38ae-aebb-42be-9693-113d36adc966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tmbxphw6gAMw",
    "outputId": "87548970-8d3f-4adc-d366-c5c3e71046ce"
   },
   "outputs": [],
   "source": [
    "# 1. Montar Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. Definir la ruta al archivo en Drive\n",
    "# Supongamos que el archivo está en \"Mi unidad/2_FACT_1_EMPRESA.csv\"\n",
    "file_path = \"/content/drive/MyDrive/Curso - Big Data EAN/2_FACT_1_EMPRESA.csv\"\n",
    "\n",
    "# Leemos el CSV en Spark\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True, sep=\";\")\n",
    "\n",
    "######################################################################\n",
    "# Lista de columnas actuales\n",
    "cols = df.columns\n",
    "\n",
    "# Generamos nombres genéricos: col1, col2, ...\n",
    "new_cols = [f\"col{i+1}\" for i in range(len(cols))]\n",
    "\n",
    "# Creamos un DataFrame con columnas renombradas\n",
    "df_renamed = df.toDF(*new_cols)\n",
    "\n",
    "# Ahora eliminamos la columna que originalmente era \"Proveedor\"\n",
    "# (en el esquema original estaba en la posición 9, o sea col9)\n",
    "df = df_renamed.drop(\"col9\")\n",
    "######################################################################\n",
    "\n",
    "# Revisamos el esquema del DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Mostramos algunas filas\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2253f59a-ab0b-4307-9adc-81e747ffe87e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "o9IJ1IiQhihC"
   },
   "source": [
    "# ⚡ Procesamiento en paralelo con Spark\n",
    "Ahora vamos a ejecutar una acción muy sencilla: contar el número de registros (`df.count()`).  \n",
    "\n",
    "En pandas:  \n",
    "- Se cargaría todo en memoria.  \n",
    "- Con datasets grandes, la máquina se puede quedar sin RAM.  \n",
    "\n",
    "En Spark:  \n",
    "- Cada **worker** cuenta una parte del dataset.  \n",
    "- El **driver** suma los resultados parciales.  \n",
    "- Esto se llama **procesamiento distribuido**.  \n",
    "\n",
    "👉 Imaginemos que tenemos una fila de cajeros contando facturas en paralelo.  \n",
    "Cada cajero cuenta un bloque, y al final el jefe suma los totales.  \n",
    "Ese “jefe” es el **Driver** de Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da26d93e-d37f-4c38-9dd4-2ff90c532712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lgZNmHXhhjVh",
    "outputId": "507c5f71-2d37-4505-8ea4-ab4ec93d6481"
   },
   "outputs": [],
   "source": [
    "# Contamos los registros de forma distribuida\n",
    "registros = df.count()\n",
    "print(\"Número total de registros:\", registros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a426617c-efd3-48ef-b742-51568911f471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "nj_J73v-htun"
   },
   "source": [
    "🎯 Ejemplo adicional: Agrupación por tipo de registro  \n",
    "Para mostrar más claro el poder de Spark, agruparemos los datos por **col5**  \n",
    "y contaremos cuántos registros existen en cada categoría.  \n",
    "\n",
    "Esto en Excel sería muy lento con más de 15 millones de filas.  \n",
    "Spark lo hace en paralelo → cada *worker* procesa un bloque y luego los une de forma eficiente.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ab70403-3948-4b27-aa59-cf1258c938b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wo58MJ8ghuzE",
    "outputId": "a2cf2808-8784-48fc-d732-1d01253ca4f9"
   },
   "outputs": [],
   "source": [
    "# Agrupar por tipo de registro y contarlas\n",
    "df_group_tipo = df.groupBy(\"col5\") \\\n",
    "                  .agg(F.count(\"*\").alias(\"total_registros\")) \\\n",
    "                  .orderBy(F.col(\"total_registros\").desc())\n",
    "\n",
    "df_group_tipo.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Guia_Pedagogica_PySpark_Funciones_CoLab",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
