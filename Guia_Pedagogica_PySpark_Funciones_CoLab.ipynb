{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🚀 Instalación y configuración de PySpark en Google Colab\n",
        "En esta primera parte vamos a instalar **PySpark** en Google Colab.  \n",
        "Esto nos permitirá simular un entorno distribuido de procesamiento de datos, sin necesidad de montar un cluster real.  \n",
        "\n",
        "Spark funciona bajo una arquitectura **Driver–Workers**:  \n",
        "- **Driver:** coordina el trabajo.  \n",
        "- **Workers:** procesan partes de los datos en paralelo.  \n",
        "\n",
        "👉 Con estas líneas de código, tendremos un mini-cluster Spark en Colab.\n"
      ],
      "metadata": {
        "id": "BDmMmD4egC24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🚀 SparkSession y funciones `F` en PySpark\n",
        "\n",
        "## 🔹 SparkSession\n",
        "En PySpark, el **punto de entrada principal** para trabajar con datos es la **SparkSession**.  \n",
        "\n",
        "- **En entornos locales (tu computador o un servidor sin Spark preconfigurado):**  \n",
        "  Necesitamos **crear la sesión** manualmente, indicando un nombre para nuestra aplicación.  \n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ClaseBigData\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "CbSy7Rdjlott"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_5q5hs1fWNy",
        "outputId": "094ca628-8797-4135-c9dd-746148638d2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Versión de Spark: 3.5.1\n"
          ]
        }
      ],
      "source": [
        "# Instalamos PySpark\n",
        "!pip install pyspark\n",
        "\n",
        "# Importamos librerías\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Creamos la sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"ClaseBigData\").getOrCreate()\n",
        "\n",
        "# Verificamos la versión\n",
        "print(\"Versión de Spark:\", spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📦 `pyspark.sql.functions as F`\n",
        "\n",
        "## 🔹 ¿Qué es `F`?\n",
        "En PySpark, la librería `pyspark.sql.functions` (comúnmente importada como `F`) es un **conjunto de funciones listas para usar** que permiten manipular columnas de un **DataFrame distribuido**.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F"
      ],
      "metadata": {
        "id": "Bc_DP-CeqEL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📌 Funciones más usadas en PySpark (`pyspark.sql.functions as F`)\n",
        "\n",
        "A continuación se presentan las principales categorías de funciones de `F` con ejemplos de uso en PySpark:\n",
        "\n",
        "# Guía de Funciones PySpark\n",
        "\n",
        "## 1. 📌 Operaciones sobre columnas\n",
        "\n",
        "Permiten crear, seleccionar o condicionar columnas.\n",
        "\n",
        "```python\n",
        "# Seleccionar una columna\n",
        "F.col(\"columna\")\n",
        "\n",
        "# Crear una columna con un valor fijo\n",
        "F.lit(100)\n",
        "\n",
        "# Condicional tipo if-else\n",
        "F.when(F.col(\"precio\") > 100, F.col(\"precio\") * 0.9).otherwise(F.col(\"precio\"))\n",
        "```\n",
        "\n",
        "**Ejemplo de uso:**\n",
        "\n",
        "```python\n",
        "df = df.withColumn(\"con_descuento\",\n",
        "                   F.when(F.col(\"precio\") > 100, F.col(\"precio\") * 0.9)\n",
        "                    .otherwise(F.col(\"precio\")))\n",
        "```\n",
        "\n",
        "## 2. 📊 Agregaciones\n",
        "\n",
        "Se usan junto a `groupBy()` y `agg()` para resumir información.\n",
        "\n",
        "```python\n",
        "F.count(\"*\")                # Contar filas\n",
        "F.sum(\"columna\")            # Suma\n",
        "F.avg(\"columna\")            # Promedio\n",
        "F.max(\"columna\")            # Máximo\n",
        "F.min(\"columna\")            # Mínimo\n",
        "F.countDistinct(\"columna\")  # Valores únicos\n",
        "```\n",
        "\n",
        "**Ejemplo de uso:**\n",
        "\n",
        "```python\n",
        "df.groupBy(\"categoria\").agg(F.avg(\"precio\").alias(\"precio_promedio\"))\n",
        "```\n",
        "\n",
        "**Comparación en Python:**\n",
        "\n",
        "```python\n",
        "df.groupby(\"categoria\")[\"precio\"].mean()\n",
        "```\n",
        "\n",
        "## 3. 🔤 Funciones de texto\n",
        "\n",
        "Se utilizan para limpiar o manipular cadenas.\n",
        "\n",
        "```python\n",
        "F.upper(\"columna\")                                # Convertir a mayúsculas\n",
        "F.lower(\"columna\")                                # Convertir a minúsculas\n",
        "F.length(\"columna\")                               # Longitud de la cadena\n",
        "F.concat(F.col(\"nombre\"), F.lit(\" \"), F.col(\"apellido\"))  # Concatenar\n",
        "```\n",
        "\n",
        "**Ejemplo de uso:**\n",
        "\n",
        "```python\n",
        "df = df.withColumn(\"nombre_completo\",\n",
        "                   F.concat(F.col(\"nombre\"), F.lit(\" \"), F.col(\"apellido\")))\n",
        "```\n",
        "\n",
        "**Comparación en Python:**\n",
        "\n",
        "```python\n",
        "df[\"nombre_completo\"] = df[\"nombre\"] + \" \" + df[\"apellido\"]\n",
        "```\n",
        "\n",
        "## 4. 📅 Funciones de fechas y tiempos\n",
        "\n",
        "Son clave para análisis temporal.\n",
        "\n",
        "```python\n",
        "F.current_date()                           # Fecha actual\n",
        "F.current_timestamp()                      # Fecha y hora actual\n",
        "F.year(\"columna_fecha\")                    # Extraer año\n",
        "F.month(\"columna_fecha\")                   # Extraer mes\n",
        "F.dayofmonth(\"columna_fecha\")              # Extraer día del mes\n",
        "F.datediff(F.current_date(), \"columna_fecha\")  # Diferencia en días\n",
        "```\n",
        "\n",
        "**Ejemplo de uso:**\n",
        "\n",
        "```python\n",
        "df = df.withColumn(\"edad_dias\",\n",
        "                   F.datediff(F.current_date(), F.col(\"fecha_nacimiento\")))\n",
        "```\n",
        "\n",
        "**Comparación en Python:**\n",
        "\n",
        "```python\n",
        "(df[\"fecha_actual\"] - df[\"fecha_nacimiento\"]).dt.days\n",
        "```\n",
        "\n",
        "## 5. ⚠️ Manejo de nulos\n",
        "\n",
        "Muy importantes para limpiar datos faltantes.\n",
        "\n",
        "```python\n",
        "F.isnull(\"columna\")                 # Detectar nulos True/False\n",
        "F.coalesce(\"columna1\", \"columna2\", \"columna3\", ...)   # Por cada fila, evalúa las columnas en orden y devuelve el primer valor que no sea nulo\n",
        "```\n",
        "\n",
        "**Ejemplo de uso:**\n",
        "\n",
        "```python\n",
        "df = df.withColumn(\"valor_final\", F.coalesce(\"columnaA\", \"columnaB\"))\n",
        "```\n",
        "\n",
        "**Comparación en Python:**\n",
        "\n",
        "```python\n",
        "df[\"valor_final\"] = df[\"columnaA\"].fillna(df[\"columnaB\"])\n",
        "```\n"
      ],
      "metadata": {
        "id": "X_vb8ppYqoHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📂 Cargando un CSV masivo en Spark  \n",
        "En esta sección vamos a cargar un dataset de **15,747,461 registros**, que en Excel o pandas sería prácticamente imposible de manejar.  \n",
        "\n",
        "Spark puede leer este archivo en paralelo, lo que significa que lo divide en **particiones** y asigna cada fragmento a un *worker*.  \n",
        "De esta manera, el procesamiento es mucho más eficiente que con herramientas tradicionales.  \n",
        "\n",
        "👉 Usaremos un CSV de 15,7 millones de filas con el siguiente esquema:"
      ],
      "metadata": {
        "id": "JiwDeVFzhXy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Definir la ruta al archivo en Drive\n",
        "# Supongamos que el archivo está en \"Mi unidad/2_FACT_1_EMPRESA.csv\"\n",
        "file_path = \"/content/drive/MyDrive/Curso - Big Data EAN/2_FACT_1_EMPRESA.csv\"\n",
        "\n",
        "# Leemos el CSV en Spark\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True, sep=\";\")\n",
        "\n",
        "######################################################################\n",
        "# Lista de columnas actuales\n",
        "cols = df.columns\n",
        "\n",
        "# Generamos nombres genéricos: col1, col2, ...\n",
        "new_cols = [f\"col{i+1}\" for i in range(len(cols))]\n",
        "\n",
        "# Creamos un DataFrame con columnas renombradas\n",
        "df_renamed = df.toDF(*new_cols)\n",
        "\n",
        "# Ahora eliminamos la columna que originalmente era \"Proveedor\"\n",
        "# (en el esquema original estaba en la posición 9, o sea col9)\n",
        "df = df_renamed.drop(\"col9\")\n",
        "######################################################################\n",
        "\n",
        "# Revisamos el esquema del DataFrame\n",
        "df.printSchema()\n",
        "\n",
        "# Mostramos algunas filas\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmbxphw6gAMw",
        "outputId": "87548970-8d3f-4adc-d366-c5c3e71046ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "root\n",
            " |-- col1: integer (nullable = true)\n",
            " |-- col2: integer (nullable = true)\n",
            " |-- col3: string (nullable = true)\n",
            " |-- col4: integer (nullable = true)\n",
            " |-- col5: string (nullable = true)\n",
            " |-- col6: string (nullable = true)\n",
            " |-- col7: string (nullable = true)\n",
            " |-- col8: integer (nullable = true)\n",
            " |-- col10: string (nullable = true)\n",
            " |-- col11: boolean (nullable = true)\n",
            "\n",
            "+-------+----+--------------------+----+-------------+--------------------+----------+----+--------------------+-----+\n",
            "|   col1|col2|                col3|col4|         col5|                col6|      col7|col8|               col10|col11|\n",
            "+-------+----+--------------------+----+-------------+--------------------+----------+----+--------------------+-----+\n",
            "|1360602|9599|\"\"\"Des Get Dreams\"\"\"|   1|  Informativa|64e6761393293143c...|2025-08-16|   0|689e24a0e4dccc46f...|false|\n",
            "|1360602|9600|  \"\"\"Des Guayacán\"\"\"|   2|  Informativa|64e6768793293143c...|2025-08-16|   0|689e25def5c996081...|false|\n",
            "|1360602|9601|\"\"\"Des Silvestre ...|   3|  Informativa|64e6768793293143c...|2025-08-16|   0|689e25e37a1cf66b6...|false|\n",
            "|1360602|1740|\"\"\"Cialta Bono 50...|   4|Transaccional|64e676ab93293143c...|      NULL|   0|5efbb6f10a29f476e...|false|\n",
            "|1360602|1755|\"\"\"Cbc Konfood 20...|   5|Transaccional|64e676ab93293143c...|      NULL|   0|5f10824c7dd91352c...|false|\n",
            "+-------+----+--------------------+----+-------------+--------------------+----------+----+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚡ Procesamiento en paralelo con Spark\n",
        "Ahora vamos a ejecutar una acción muy sencilla: contar el número de registros (`df.count()`).  \n",
        "\n",
        "En pandas:  \n",
        "- Se cargaría todo en memoria.  \n",
        "- Con datasets grandes, la máquina se puede quedar sin RAM.  \n",
        "\n",
        "En Spark:  \n",
        "- Cada **worker** cuenta una parte del dataset.  \n",
        "- El **driver** suma los resultados parciales.  \n",
        "- Esto se llama **procesamiento distribuido**.  \n",
        "\n",
        "👉 Imaginemos que tenemos una fila de cajeros contando facturas en paralelo.  \n",
        "Cada cajero cuenta un bloque, y al final el jefe suma los totales.  \n",
        "Ese “jefe” es el **Driver** de Spark.\n"
      ],
      "metadata": {
        "id": "o9IJ1IiQhihC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contamos los registros de forma distribuida\n",
        "registros = df.count()\n",
        "print(\"Número total de registros:\", registros)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgZNmHXhhjVh",
        "outputId": "507c5f71-2d37-4505-8ea4-ab4ec93d6481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número total de registros: 15747461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎯 Ejemplo adicional: Agrupación por tipo de registro  \n",
        "Para mostrar más claro el poder de Spark, agruparemos los datos por **col5**  \n",
        "y contaremos cuántos registros existen en cada categoría.  \n",
        "\n",
        "Esto en Excel sería muy lento con más de 15 millones de filas.  \n",
        "Spark lo hace en paralelo → cada *worker* procesa un bloque y luego los une de forma eficiente.  "
      ],
      "metadata": {
        "id": "nj_J73v-htun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupar por tipo de registro y contarlas\n",
        "df_group_tipo = df.groupBy(\"col5\") \\\n",
        "                  .agg(F.count(\"*\").alias(\"total_registros\")) \\\n",
        "                  .orderBy(F.col(\"total_registros\").desc())\n",
        "\n",
        "df_group_tipo.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo58MJ8ghuzE",
        "outputId": "a2cf2808-8784-48fc-d732-1d01253ca4f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------------+\n",
            "|         col5|total_registros|\n",
            "+-------------+---------------+\n",
            "|  Informativa|       12620939|\n",
            "|Transaccional|        3126522|\n",
            "+-------------+---------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}